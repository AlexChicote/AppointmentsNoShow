# AppointmentsNoShow*****TechnicalReport

The goal of this project is try to predict the show/Noshow in a medical appointment. Thus, this is a binary classification model that I will have to develop. Starting with data:

1. I obtained the data from 2 sources: One Kaggle from where I obtained the information about 110K medical appointments from Vitoria, Brazil. This data contain 13 characteristics about the appointment. Some of them are about the appointment itself: AppointmentId, Scheduled Date and Time, Appointment Date (no time, what makes me suspect that the time from scheduled is actually from appointment), Neighborhood where it took place and if the patient received a cell phone message reminder. Others are more about the patient: PatientId, Age, Gender, Diabetes, Hyoertension, Alcoholism, Handicaps and Scholarship (what is a type of subsidy).
The other source has been www.wunderground.com from where I scraped weather information from Vitoria airport for the dates of the appointments (April 29th to June 8th 2016). Temp, winfd and special events (nothing, fog, rain and thunderstorm) were the features selected from the weather.
2. Feature Engineering. This has been an interesting and fruitful part as some of the features generated in this phase, ended up being some of the most important.
Patient_repeat, number of appointments missed. This two were trying to capture the idea that people visiting more often medical facilities are more likely to miss appointments(patient_repeat) and that patients that missed appointments before are more prone to do it again(num_missed_appointments).
I was able also to extract from the AppointmentDate field the day of the week and the month as a separate field with the idea that the latter might be very usefuk in ghe future if I ever got to do part 2 and add the dataset with appointments from 2015 and 2014. I just want to mention the feature "session". It is the part of the day in which the appointment was scheduled. I never thought it was going to be a relevant but I admit that had gthe suspicion (without any proof to back it) that it might actually be the time of the appointment itself. Why? Because it is strange to me that we have the data from the Scheduling (totally irrelevant) and do not dipose the one from the appointment. Said that, it would not be the first time that something this strange happens.
3. Model and feature selection. Once I was done with features and had gotten some dummies out of the Neighborhood field (81 of them), I ended uo qith 109  features. I loop over the model adding features in the loop: from 2 to 109 using correlaction or selectkbest in some otther case. This way I solved two problems: I selected the model and knew the best number of features. The winner was GradientBoost Classifier that always got the best score (metric; accuracy) and beat the baseline (79.8%).
4. The best score I obtained was 80.7% and it included 5 features: awaiting_time, age and num_app_missed as really important with session and SMS-received far behind. I called that the victory of the common sense: what the person working in a medical center the last 10 years will say: "You had to do all this to find out something I already knew". 'Yes, you were righ in your suspicious but now you are right in your knowledge", I would reply.
The score 0f 80.7% apart from a the social relevance is also a way to save resources in a very neede it field like the socialise medicine and it comes  with a False Positive Rate (what about people showing up that you did not expect!!!) is 0.009 or 0.9% or, if we want to put it as the oposite of an specificity of 99.1%
MORE COMING!!!!
5. As part of my modeling, I started working with hyperparameters and other tools ghat could imporve my score. I tried SMOTE (a way to deal with imbalnce sets) as well as PCA but they did not improve my scores. While I am open to keep tweaking the model and I have played a bit with the parameters of my model, I have not gotten any positive results.
NOT GIVING UP!!!
6. I want to keep on moving forward in this project so I will try more the tunning of my model and I also want to try BaggingClassifier and a Bayes approach to the project.
7. It is also my idea to add the dataset that already exists from 2014 and 2015 what presents certain difficulties: First, some of the features (important ones in 2016) are not present in 2014 and 2015. SECOND. while my baseline is 79.8%, the baseline for the ogher years is barely 70% that with the fact thet i have only 300k for a much longer period of time, makes me think that is not that consistent the dataset.

I will keep you posted.

Enjoy the Classico (if you support Messi's team) and good luck the rest of the holidays if you don't 

